{"version":3,"sources":["../../../lib/compiler/tokenizer.js"],"names":["tokens","matchPattern","require","Text","getTopLevelTokens","Object","keys","map","key","filter","token","priority","sort","a","b","findOpensAndCloses","output","opens","closes","forEach","tokenType","startsWith","push","removeExtraCloses","remove","Set","closeSubject","expectedSubjects","index","subject","path","test","raw","expectedSubject","length","pop","add","matches","match","i","tok","m","has","tokenizer","input","topLevelTokens","cursor","lastBreak","slice","found","text","escapedText","Tok","module","exports"],"mappings":"AAAA;;AAEA,MAAM,EAAEA,MAAF,EAAUC,YAAV,KAA2BC,QAAQ,UAAR,CAAjC;;AAEA,MAAM,EAAEC,IAAF,KAAWH,MAAjB;;AAEA,SAASI,iBAAT,GAA6B;AAC3B,SAAOC,OAAOC,IAAP,CAAYN,MAAZ,EACJO,GADI,CACAC,OAAOR,OAAOQ,GAAP,CADP,EAEJC,MAFI,CAEGC,SAASA,MAAMC,QAAN,GAAiB,CAF7B,EAGJC,IAHI,CAGC,CAACC,CAAD,EAAIC,CAAJ,KAAUD,EAAEF,QAAF,GAAaG,EAAEH,QAH1B,CAAP;AAID;;AAED,SAASI,kBAAT,CAA4BC,MAA5B,EAAoC;AAClC,QAAMC,QAAQ,EAAd;AACA,QAAMC,SAAS,EAAf;AACAF,SAAOG,OAAP,CAAgBT,KAAD,IAAW;AACxB,QAAIA,MAAMU,SAAN,CAAgBC,UAAhB,CAA2B,MAA3B,CAAJ,EAAwC;AACtCJ,YAAMK,IAAN,CAAWZ,KAAX;AACD,KAFD,MAEO,IAAIA,MAAMU,SAAN,KAAoB,OAAxB,EAAiC;AACtCF,aAAOI,IAAP,CAAYZ,KAAZ;AACD;AACF,GAND;;AAQA,SAAO,CAACO,KAAD,EAAQC,MAAR,CAAP;AACD;;AAED,SAASK,iBAAT,CAA2BP,MAA3B,EAAmC;AACjC,QAAMQ,SAAS,IAAIC,GAAJ,EAAf;AACA,QAAMC,eAAe,4BAArB;;AAEA,QAAMC,mBAAmB,EAAzB;AACA;AACAX,SAAOG,OAAP,CAAe,CAACT,KAAD,EAAQkB,KAAR,KAAkB;AAC/B,QAAIlB,MAAMU,SAAN,CAAgBC,UAAhB,CAA2B,MAA3B,CAAJ,EAAwC;AACtCM,uBAAiBL,IAAjB,CACGZ,MAAMmB,OAAN,IAAiBnB,MAAMmB,OAAN,CAAcC,IAAhC,IACCpB,MAAMqB,IAAN,KAAerB,MAAMqB,IAAN,CAAWC,GAAX,IAAkBtB,MAAMqB,IAAN,CAAWD,IAA5C,CAFH;AAID,KALD,MAKO,IAAIpB,MAAMU,SAAN,KAAoB,OAAxB,EAAiC;AACtC,YAAMa,kBAAkBN,iBAAiBA,iBAAiBO,MAAjB,GAA0B,CAA3C,CAAxB;AACAP,uBAAiBQ,GAAjB;;AAEA,UAAI,CAACF,eAAL,EAAsB;AACpBT,eAAOY,GAAP,CAAW1B,KAAX;AACD,OAFD,MAEO;AACL,cAAM2B,UAAU3B,MAAMsB,GAAN,CAAUM,KAAV,CAAgBZ,YAAhB,CAAhB;AACA,YAAIW,WAAWA,QAAQ,CAAR,MAAeJ,eAA9B,EAA+C;AAC7CT,iBAAOY,GAAP,CAAW1B,KAAX;AACD,SAFD,MAEO;AACL;AACA;AACA,eAAK,IAAI6B,IAAIX,QAAQ,CAArB,EAAwBW,IAAIvB,OAAOkB,MAAnC,EAA2CK,KAAK,CAAhD,EAAmD;AACjD,kBAAMC,MAAMxB,OAAOuB,CAAP,CAAZ;AACA,gBAAIC,IAAIpB,SAAJ,CAAcC,UAAd,CAAyB,MAAzB,CAAJ,EAAsC;AACpC;AACD;AACD,gBAAImB,IAAIpB,SAAJ,KAAkB,OAAtB,EAA+B;AAC7B,oBAAMqB,IAAID,IAAIR,GAAJ,CAAQM,KAAR,CAAcZ,YAAd,CAAV;AACA,kBAAIe,KAAKA,EAAE,CAAF,MAASR,eAAlB,EAAmC;AACjC;AACAT,uBAAOY,GAAP,CAAW1B,KAAX;AACA;AACD;AACF;AACF;AACF;AACF;AACF;AACF,GApCD;;AAsCA,SAAOM,OAAOP,MAAP,CAAcC,SAAS,CAACc,OAAOkB,GAAP,CAAWhC,KAAX,CAAxB,CAAP;AACD;;AAED;;;;;AAKA,SAASiC,SAAT,CAAmBC,KAAnB,EAA0B;AACxB,QAAMC,iBAAiBzC,mBAAvB;;AAEA,QAAM8B,SAASU,MAAMV,MAArB;;AAEA,QAAMlB,SAAS,EAAf;;AAEA,MAAI8B,SAAS,CAAb;AACA,MAAIC,YAAY,CAAhB;;AAEA,SAAOD,SAASZ,MAAhB,EAAwB;AACtB,UAAMc,QAAQJ,MAAMI,KAAN,CAAYF,MAAZ,CAAd;AACA,UAAMG,QAAQhD,aAAa4C,cAAb,EAA6BG,KAA7B,EAAoC,KAApC,CAAd;;AAEA,QAAIC,SAASL,MAAME,SAAS,CAAf,MAAsB,IAAnC,EAAyC;AACvC,YAAMI,OAAON,MAAMI,KAAN,CAAYD,SAAZ,EAAuBD,SAAS,CAAhC,CAAb;AACA,UAAII,IAAJ,EAAU;AACRlC,eAAOM,IAAP,CAAY,IAAInB,IAAJ,CAAS+C,IAAT,CAAZ;AACD;;AAED,YAAMC,cAAcF,MAAM,CAAN,EAAS,CAAT,CAApB;AACAjC,aAAOM,IAAP,CAAY,IAAInB,IAAJ,CAASgD,WAAT,CAAZ;;AAEAL,gBAAUK,YAAYjB,MAAtB;AACAa,kBAAYD,MAAZ;AACD,KAXD,MAWO,IAAIG,KAAJ,EAAW;AAChB,YAAM,CAACG,GAAD,EAAMf,OAAN,IAAiBY,KAAvB;;AAEA,YAAMC,OAAON,MAAMI,KAAN,CAAYD,SAAZ,EAAuBD,MAAvB,CAAb;AACA,UAAII,IAAJ,EAAU;AACRlC,eAAOM,IAAP,CAAY,IAAInB,IAAJ,CAAS+C,IAAT,CAAZ;AACD;AACDlC,aAAOM,IAAP,CAAY,IAAI8B,GAAJ,CAAQ,GAAGf,OAAX,CAAZ;;AAEAS,gBAAUT,QAAQ,CAAR,EAAWH,MAArB;AACAa,kBAAYD,MAAZ;AACD,KAXM,MAWA;AACLA,gBAAU,CAAV;AACD;AACF;AACD,QAAMI,OAAON,MAAMI,KAAN,CAAYD,SAAZ,EAAuBD,MAAvB,CAAb;AACA,MAAII,IAAJ,EAAU;AACRlC,WAAOM,IAAP,CAAY,IAAInB,IAAJ,CAAS+C,IAAT,CAAZ;AACD;;AAED,QAAM,CAACjC,KAAD,EAAQC,MAAR,IAAkBH,mBAAmBC,MAAnB,CAAxB;;AAEA;AACA;AACA,MAAIE,OAAOgB,MAAP,GAAgBjB,MAAMiB,MAA1B,EAAkC;AAChC,WAAOX,kBAAkBP,MAAlB,EAA0BC,KAA1B,EAAiCC,MAAjC,CAAP;AACD;;AAED,SAAOF,MAAP;AACD;;AAEDqC,OAAOC,OAAP,GAAiBX,SAAjB","file":"tokenizer.js","sourcesContent":["'use strict';\n\nconst { tokens, matchPattern } = require('./tokens');\n\nconst { Text } = tokens;\n\nfunction getTopLevelTokens() {\n  return Object.keys(tokens)\n    .map(key => tokens[key])\n    .filter(token => token.priority > 0)\n    .sort((a, b) => a.priority - b.priority);\n}\n\nfunction findOpensAndCloses(output) {\n  const opens = [];\n  const closes = [];\n  output.forEach((token) => {\n    if (token.tokenType.startsWith('Open')) {\n      opens.push(token);\n    } else if (token.tokenType === 'Close') {\n      closes.push(token);\n    }\n  });\n\n  return [opens, closes];\n}\n\nfunction removeExtraCloses(output) {\n  const remove = new Set();\n  const closeSubject = /^<!-- END[^ ]* !?(.*) -->$/;\n\n  const expectedSubjects = [];\n  // try to find a Close with no corresponding Open\n  output.forEach((token, index) => {\n    if (token.tokenType.startsWith('Open')) {\n      expectedSubjects.push(\n        (token.subject && token.subject.path) ||\n        (token.test && (token.test.raw || token.test.path))\n      );\n    } else if (token.tokenType === 'Close') {\n      const expectedSubject = expectedSubjects[expectedSubjects.length - 1];\n      expectedSubjects.pop();\n\n      if (!expectedSubject) {\n        remove.add(token);\n      } else {\n        const matches = token.raw.match(closeSubject);\n        if (matches && matches[1] !== expectedSubject) {\n          remove.add(token);\n        } else {\n          // search for a close within close proximity\n          // that has the expected subject\n          for (let i = index + 1; i < output.length; i += 1) {\n            const tok = output[i];\n            if (tok.tokenType.startsWith('Open')) {\n              break;\n            }\n            if (tok.tokenType === 'Close') {\n              const m = tok.raw.match(closeSubject);\n              if (m && m[1] === expectedSubject) {\n                // found one ahead, so remove the current one\n                remove.add(token);\n                break;\n              }\n            }\n          }\n        }\n      }\n    }\n  });\n\n  return output.filter(token => !remove.has(token));\n}\n\n/**\n * Generate an array of tokens describing the template\n * @param {string} input\n * @return {Token[]}\n */\nfunction tokenizer(input) {\n  const topLevelTokens = getTopLevelTokens();\n\n  const length = input.length;\n\n  const output = [];\n\n  let cursor = 0;\n  let lastBreak = 0;\n\n  while (cursor < length) {\n    const slice = input.slice(cursor);\n    const found = matchPattern(topLevelTokens, slice, false);\n\n    if (found && input[cursor - 1] === '\\\\') {\n      const text = input.slice(lastBreak, cursor - 1);\n      if (text) {\n        output.push(new Text(text));\n      }\n\n      const escapedText = found[1][0];\n      output.push(new Text(escapedText));\n\n      cursor += escapedText.length;\n      lastBreak = cursor;\n    } else if (found) {\n      const [Tok, matches] = found;\n\n      const text = input.slice(lastBreak, cursor);\n      if (text) {\n        output.push(new Text(text));\n      }\n      output.push(new Tok(...matches));\n\n      cursor += matches[0].length;\n      lastBreak = cursor;\n    } else {\n      cursor += 1;\n    }\n  }\n  const text = input.slice(lastBreak, cursor);\n  if (text) {\n    output.push(new Text(text));\n  }\n\n  const [opens, closes] = findOpensAndCloses(output);\n\n  // if there are more closes than opens\n  // intelligently remove extra ones\n  if (closes.length > opens.length) {\n    return removeExtraCloses(output, opens, closes);\n  }\n\n  return output;\n}\n\nmodule.exports = tokenizer;\n"]}